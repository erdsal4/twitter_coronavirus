# Coronavirus twitter analysis

## Background

In this project, we use the MapReduce technique to analyze coronavirus related tweets by country and by language among all geotagged tweets between October 2019 and April 2020. We do this by calculating the number of tweets that included one or more hashtags from a predetermined list of hashtags like `#covid19`, `#coronavirus` etc. Since our data is multilingual, we also include some hashtag keywords in Chinese and Japanese. Since there are millions of tweets, the MapReduce technique provides us with an efficient way to count these tweets.

**Learning Objectives:**

1. work with large scale datasets
1. work with multilingual text
1. use the MapReduce divide-and-conquer paradigm to create parallel code

**Runtime:**

In our case, the map procedure takes time O(n) and the reduce procedure takes time O(1).
If we have p<<n processors, then the overall runtime will be O(n/p).
This means that:
1. doubling the amount of data will cause the analysis to take twice as long;
1. doubling the number of processors will cause the analysis to take half as long;
1. if we want to add more data and keep the processing time the same, then we need to add a proportional number of processors.

## Running the code

If you like to try this analysis on your own set of data, here is a list of commands that can be helpful in doing so.

1. **Mapping:**
   The `map.py` file processes a single zip file of tweets.
   From the root directory of your clone, run the command
   ```
   $ ./src/map.py --input_path=/<PATH_TO_DATASET>/geoTwitter20-02-16.zip
   ```
   This command will take a few minutes to run as it is processing all of the tweets within the zip file.
   After the command finishes, you will now have a folder `outputs` that contains a file `geoTwitter20-02-16.zip.lang`.
   This is a file that contains JSON formatted information summarizing the tweets from 16 February 2020.

1. **Visualizing:**
   The `visualize.py` file displays the output from running the `map.py` file.
   Run the command
   ```
   $ ./src/visualize.py --input_path=outputs/geoTwitter20-02-16.zip.lang --key='#coronavirus'
   ```
   This displays the total number of times the hashtag `#coronavirus` was used on 16 February in each of the languages supported by twitter.

   The file `geoTwitter20-02-16.zip.lang` contains a dictionary of dictionaries.
   The outermost dictionary has languages as the keys, 
   and the innermost dictionary has hashtags as the keys.
   The `visualize.py` file simply provides a nicer visualization of these dictionaries.

1. **Reducing:**
   The `reduce.py` file merges the outputs generated by the `map.py` file so that the combined files can be visualized.
   Generate a new output file by running the command
   ```
   $ ./src/map.py --input_path=/<PATH_TO_DATASET>/geoTwitter20-02-17.zip
   ```
   Then merge these output files together by running the command
   ```
   $ ./src/reduce.py --input_paths outputs/geoTwitter20-02-16.zip.lang outputs/geoTwitter20-02-17.zip.lang --output_path=reduced.lang
   ```
   Alternatively, you can use the glob to merge all output files with the command
   ```
   $ ./src/reduce.py --input_paths outputs/geoTwitter*.lang --output_path=reduced.lang
   ```
   Now you can visualize the `reduced.lang` file with the command
   ```
   $ ./src/visualize.py --input_path=reduced.lang --key='#coronavirus'
   ```
   and this displays the combined result.

## Results

### By Country

The `viz/country` folder contains our results for the number of tweets using the hashtags by country between October 2019-April 2020. For example, the `#covid19` hashtag has the following counts:

```
US : 88548
GB : 26236
IN : 23870
CA : 16654
ES : 12025
NG : 11344
FR : 10226
TR : 10162
ZA : 10106
IT : 9270
```

We see that the US has the most number of tweets with this hashtag over this period, followed by Great Britain and India. These countries were the hotspots of COVID-19, so it makes sense that they are the ones that have the most tweets with this hashtag. As another example, we look at the tweets with hashtag `#冠状病毒`, which is Chinese for "coronavirus". 

```
US : 23
CN : 21
JP : 7
SG : 3
ES : 3
TW : 2
PH : 2
AE : 2
MV : 1
MN : 1
```

We were very surprised by the result that only 21 tweets included this hashtag from China throughout this period, when China was actually the center of Covid-19 cases. Even the US has more tweets with this hashtag. This could be due to the fact that Twitter is blocked in China, and that the authorities might have been closely engaged in supressing tweets about Covid-19. 

### By language

We now look at the number of tweets using the hashtags by language. As a good example, we look at the counts for `#corona`:

```
en : 340693
es : 90643
und : 89650
tr : 34672
it : 24938
pt : 21676
fr : 19643
hi : 16927
de : 14369
in : 12913
```

We are not surprised to see that English is the lanugage with the most tweet count, since English is a widely used language in the world. The following languages like "Spanish" and "Turkish" might reflect the high engagement and interest of users with the pandemic from Spanish and Turkish speaking countries, or relate to an increase in cases in these countries. Although Spain and Italy were early hotspots for the pandemic, we wouldn't expect people from Turkey to be talking about Covid as much, since Turkey was not a hotspot for the pandemic. It is also worth noting that the languages of a huge number(89650) of tweets using this hashtag could not be identified, which shows how language detection algorithms might fail to detect some languages, in turn causing less utilization of tweet data for the citizens of those countries. 

As another example, we look at the counts for the hashtag `#hospital`:

```
en : 40572
es : 2107
und : 1200
pt : 287
ca : 207
hu : 188
in : 173
fr : 122
ja : 63
hi : 59
```

We see that this specific hashtag has a bias towards being used in English tweets, since the word itself is English. We are suprised to see a high number of tweets mentioning `#hospital`. The tweets using this hashtag could be from health care providers and news channels tweeting about hospital utilization rates during these stages of the pandemic. Tweets with this hashtag could have provided a lot of insight for policy makers who would want to see where hospitals are running out of equipment and capacity etc. 


